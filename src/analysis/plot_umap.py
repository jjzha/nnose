#!/usr/bin/env python
# -*- coding:utf-8 -*-
# @Filename:    plot_umap.py
# @Time:        17/05/2023 11.51

import argparse
import logging
import os
import time
from typing import Tuple

import matplotlib.pyplot as plt
import numpy as np
import torch
import tqdm
import umap

RAW_FEATURE_KEY_SUFFIX = ".pt"
RAW_FEATURE_VALUE_SUFFIX = "_values.pt"
RAW_FEATURE_TOKEN_SUFFIX = "_tokens.pt"

# set up logger
logger = logging.getLogger(__name__)
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)

import pickle


def read_feature_files(feature_dir: str, percentage: int = 100) -> Tuple:
    """
    Read the raw features generated by generate_raw_feature.py, and stack them into on single tensor.
    :param feature_dir: The directory containing the raw features.
    :param percentage: The percentage of files to read from (mainly for testing purpose).
    :return:
    key_store: a numpy array of shape (num_keys, dim_keys), each row is a key
    label_id_store: a numpy array of shape (num_keys, 1), each row represents the value (target token) to the key.
    """
    value_files = list(
        filter(lambda x: x.endswith(RAW_FEATURE_VALUE_SUFFIX), os.listdir(feature_dir))
    )
    value_files = value_files[: int(len(value_files) * (percentage / 100.0))]
    key_store = []
    label_id_store = []
    input_id_store = []
    start_time = time.time()
    dataset_count = {"skillspan": 0, "sayfullina": 0, "green": 0}
    for file_name in tqdm.tqdm(
        value_files, total=len(value_files), desc="Loading feature files"
    ):
        file_id = file_name.split(RAW_FEATURE_VALUE_SUFFIX)[0]
        key_path = os.path.join(feature_dir, str(file_id) + RAW_FEATURE_KEY_SUFFIX)
        value_path = os.path.join(feature_dir, str(file_id) + RAW_FEATURE_VALUE_SUFFIX)
        input_id_path = os.path.join(
            feature_dir, str(file_id) + RAW_FEATURE_TOKEN_SUFFIX
        )
        dataset_name = file_id.split("_")[2]

        try:
            curr_keys = torch.load(key_path)
            curr_label_ids = torch.load(value_path)
            curr_input_ids = torch.load(input_id_path)
        except Exception as e:
            logger.error(f"Failed to load {key_path} or {value_path}.")
            raise IOError(e)
        key_store += (
            curr_keys.cpu()
        )  # ensure that it is on CPU, as numpy doesn't support GPU
        dataset_count[dataset_name] += len(curr_label_ids)
        label_id_store += curr_label_ids.cpu()
        input_id_store += curr_input_ids.cpu()
    key_store = np.stack(key_store)
    label_id_store = np.stack(label_id_store)
    input_id_store = np.stack(input_id_store)

    logger.info(
        f"{len(key_store)} keys and values, used {time.time() - start_time} seconds"
    )
    return key_store, label_id_store, input_id_store, dataset_count


def main(args):
    f_names = ["umap_skillspan.sav", "umap_sayfullina.sav", "umap_green.sav"]
    suffix = ["skillspan_AD", "sayfullina_AD", "green_AD"]

    for idx, suf in enumerate(suffix):
        if f_names[idx] not in os.listdir(args.output_dir):
            key_store, label_id_store, input_id_store, dataset_count = read_feature_files(
                    f"datastore_100_{suf}/saved_embedding_jobberta" # can be another model as well.
                    )
            X = key_store
            logger.info(f"Could not find UMAP estimator for {suf}, applying UMAP...")
            X_umap = umap.UMAP(
                n_neighbors=100, min_dist=0.0, metric="euclidean", unique=True
            ).fit_transform(X)
            pickle.dump(X_umap, open(args.output_dir + f_names[idx], "wb"))
        else:
            continue

    logger.info("Found pre-trained UMAP estimators, loading UMAP...")
    X_umap_skillspan = pickle.load(open(args.output_dir + f_names[0], "rb"))
    X_umap_sayfullina = pickle.load(open(args.output_dir + f_names[1], "rb"))
    X_umap_green = pickle.load(open(args.output_dir + f_names[2], "rb"))

    # Plot the UMAP representation
    logger.info("Plotting UMAP...")

    # Dumb way to get indices of datasets:
    labels = np.repeat(
        [0, 1, 2],
        [
            dataset_count["green"],
            dataset_count["sayfullina"],
            dataset_count["skillspan"],
        ],
    )
    array_length = len(labels)
    array = np.zeros(array_length, dtype=int)
    array[: len(labels)] = labels
    mask = label_id_store < 2
    new_labels = labels[mask]

    kept_indices = np.where(mask)[0]

    logger.info("Getting other datapoints")
    green = []
    sayfullina = []
    skillspan = []
    for file in os.listdir(args.output_dir):
        if file.endswith("all.out"):
            with open(args.output_dir + file) as f:
                if "green" in file:
                    for i in f:
                        green += i.strip().split()
                elif "sayfullina" in file:
                    for i in f:
                        sayfullina += i.strip().split()
                else:
                    for i in f:
                        skillspan += i.strip().split()

    green = [int(i) for i in green if int(i) in kept_indices]
    sayfullina = [int(i) for i in sayfullina if int(i) in kept_indices]
    skillspan = [int(i) for i in skillspan if int(i) in kept_indices]

    fig, ax = plt.subplots(figsize=(9, 3), ncols=3, nrows=1)
    scatter = ax[0].scatter(
        X_umap_skillspan[kept_indices, 0],
        X_umap_skillspan[kept_indices, 1],
        alpha=0.35,
        c=new_labels,
        cmap="Accent",
        s=0.1,
        label="Dataset",
    )
    handles, labels_scatter = scatter.legend_elements()
    ax[0].set_title("UMAP Visualization (SkillSpan)", alpha=0.6, fontsize=10)
    ax[0].legend(handles, ["Green", "Sayfullina", "SkillSpan"], fontsize=9)

    scatter = ax[1].scatter(
        X_umap_sayfullina[kept_indices, 0],
        X_umap_sayfullina[kept_indices, 1],
        alpha=0.35,
        c=new_labels,
        cmap="Accent",
        s=0.1,
        label="Dataset",
    )
    handles, labels_scatter = scatter.legend_elements()
    ax[1].set_title("UMAP Visualization (Sayfullina)", alpha=0.6, fontsize=10)
    ax[1].legend(handles, ["Green", "Sayfullina", "SkillSpan"], fontsize=9)

    scatter = ax[2].scatter(
        X_umap_green[kept_indices, 0],
        X_umap_green[kept_indices, 1],
        alpha=0.35,
        c=new_labels,
        cmap="Accent",
        s=0.1,
        label="Dataset",
        edgecolor=None,
    )
    handles, labels_scatter = scatter.legend_elements()
    ax[2].set_title("UMAP Visualization (Green)", alpha=0.6, fontsize=10)
    ax[2].legend(handles, ["Green", "Sayfullina", "SkillSpan"], fontsize=9)

    # Highlight specific datapoints
    ax[2].scatter(
        X_umap_green[green, 0],
        X_umap_green[green, 1],
        edgecolor=None,
        c="orange",
        marker="s",
        s=0.1,
        alpha=0.1,
    )
    ax[1].scatter(
        X_umap_sayfullina[sayfullina, 0],
        X_umap_sayfullina[sayfullina, 1],
        edgecolor=None,
        c="orange",
        marker="s",
        s=0.1,
        alpha=0.1,
    )
    ax[0].scatter(
        X_umap_skillspan[skillspan, 0],
        X_umap_skillspan[skillspan, 1],
        edgecolor=None,
        c="orange",
        marker="s",
        s=0.1,
        alpha=0.1,
    )
    # Show the plot
    plt.tight_layout()
    plt.savefig(args.output_dir + "umap_plot_all_test_accent.png", dpi=1200)


def parse_args():
    parser = argparse.ArgumentParser(
        description="Generate raw feature tensors for building the datastore"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        required=True,
        help="the directory to save the files",
    )
    args = parser.parse_args()

    return args


if __name__ == "__main__":
    args = parse_args()
    main(args)
